{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.get_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:15:22.436152Z",
     "iopub.status.busy": "2025-08-15T20:15:22.435872Z",
     "iopub.status.idle": "2025-08-15T20:15:22.447344Z",
     "shell.execute_reply": "2025-08-15T20:15:22.446536Z",
     "shell.execute_reply.started": "2025-08-15T20:15:22.436128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Input,\n",
    "    GlobalAveragePooling1D,\n",
    "    MultiHeadAttention,\n",
    "    LayerNormalization,\n",
    "    Add,\n",
    ")\n",
    "\n",
    "\n",
    "def get_machine_learning_models():\n",
    "    return {\n",
    "        \"Logistic Regression\": LogisticRegression(\n",
    "            max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\"\n",
    "        ),\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            n_estimators=200, class_weight=\"balanced\"\n",
    "        ),\n",
    "        \"Naive Bayes\": MultinomialNB(),\n",
    "        \"XGBoost\": XGBClassifier(objective=\"multi:softmax\", use_label_encoder=False),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_deep_learning_models(\n",
    "    vocab_size=20000, max_len=500, embed_dim=128, num_heads=4, num_classes=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of deep learning models with their names as keys.\n",
    "    \"\"\"\n",
    "\n",
    "    if num_classes is None:\n",
    "        raise ValueError(\"num_classes must be specified for deep learning models\")\n",
    "\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    # 1. Simple Feedforward\n",
    "    models[\"Simple Feedforward\"] = Sequential(\n",
    "        [\n",
    "            Dense(256, activation=\"relu\", input_shape=(max_len,)),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 2. LSTM Model\n",
    "    lstm_input = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(lstm_input)\n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    models[\"Stacked LSTM\"] = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    # 3. GRU Model\n",
    "    gru_input = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(gru_input)\n",
    "    x = GRU(64)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    models[\"GRU\"] = Model(inputs=gru_input, outputs=output)\n",
    "\n",
    "    # 4. Transformer-like Model (simple attention block)\n",
    "    trans_input = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(trans_input)\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    models[\"Transformer\"] = Model(inputs=trans_input, outputs=output)\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests/evaluate_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:15:22.449089Z",
     "iopub.status.busy": "2025-08-15T20:15:22.448512Z",
     "iopub.status.idle": "2025-08-15T20:15:22.467323Z",
     "shell.execute_reply": "2025-08-15T20:15:22.466659Z",
     "shell.execute_reply.started": "2025-08-15T20:15:22.449070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_seq_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the given model on the test data and returns the accuracy.\n",
    "    \"\"\"\n",
    "    model_name = model.name if isinstance(model, Model) else model.__class__.__name__\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    predictions = model.predict(X_seq_test)\n",
    "    if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n",
    "        # For multi-class classification, take the class with the highest probability\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        # For binary classification, predictions are already in the correct format\n",
    "        predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "    test_labels = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src.train_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:15:22.468659Z",
     "iopub.status.busy": "2025-08-15T20:15:22.468375Z",
     "iopub.status.idle": "2025-08-15T20:15:22.482019Z",
     "shell.execute_reply": "2025-08-15T20:15:22.481502Z",
     "shell.execute_reply.started": "2025-08-15T20:15:22.468633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(\n",
    "    X_tfidf_train, X_tfidf_test, X_seq_train, X_seq_test, y_train, y_test, num_classes\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and returns a dictionary of machine learning and deep learning models.\n",
    "    \"\"\"\n",
    "\n",
    "    ml_models = get_machine_learning_models()\n",
    "    dl_models = get_deep_learning_models(num_classes=num_classes)\n",
    "    accuracy_results = {}\n",
    "\n",
    "    # Train machine learning models on TF-IDF features\n",
    "    for name, model in ml_models.items():\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "        model.fit(X_tfidf_train, y_train)\n",
    "        print(f\"{name} model training completed.\")\n",
    "        print(f\"Training Accuracy: {model.score(X_tfidf_train, y_train):.4f}\")\n",
    "\n",
    "        # Evaluate training accuracy\n",
    "        accuracy = model.score(X_tfidf_test, y_test)\n",
    "        print(f\"{name} Evaluation Accuracy: {accuracy:.4f}\")\n",
    "        accuracy_results[name] = accuracy\n",
    "\n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "    # Train deep learning models on sequence data\n",
    "    for name, model in dl_models.items():\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        # if the saved_models directory does not exist, create it\n",
    "        tf.io.gfile.makedirs(\"./saved_models\")\n",
    "\n",
    "        # Use ModelCheckpoint to save the best model during training\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"./saved_models/{name}_best_model.h5\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "        )\n",
    "\n",
    "        # Add verbose output for deep learning models\n",
    "        history = model.fit(\n",
    "            X_seq_train,\n",
    "            y_train_onehot,\n",
    "            validation_split=0.1,\n",
    "            epochs=10,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            batch_size=32,\n",
    "            verbose=1,  # Set verbose=1 to print progress for each epoch\n",
    "        )\n",
    "        print(f\"{name} model training completed.\")\n",
    "        # Print training accuracy for each epoch\n",
    "        for epoch, acc in enumerate(history.history[\"accuracy\"], 1):\n",
    "            print(f\"Epoch {epoch}: Training Accuracy = {acc:.4f}\")\n",
    "\n",
    "        # Evaluate training accuracy\n",
    "        accuracy = evaluate_model(model, X_seq_test, y_test)\n",
    "        print(f\"{name} Evaluation Accuracy: {accuracy:.4f}\")\n",
    "        accuracy_results[name] = accuracy\n",
    "\n",
    "    print(\"Evaluation Accuracy Results:\", accuracy_results)\n",
    "\n",
    "    return {**ml_models, **dl_models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src.preprocess_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:15:22.571942Z",
     "iopub.status.busy": "2025-08-15T20:15:22.571331Z",
     "iopub.status.idle": "2025-08-15T20:15:22.580904Z",
     "shell.execute_reply": "2025-08-15T20:15:22.580237Z",
     "shell.execute_reply.started": "2025-08-15T20:15:22.571917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.dropna().drop_duplicates()\n",
    "\n",
    "    feature_cols = [\"link\", \"headline\", \"short_description\", \"authors\", \"date\"]\n",
    "    X = df[feature_cols].astype(str).agg(\" \".join, axis=1)\n",
    "    X = clean_text(X)\n",
    "\n",
    "    category_to_idx = {k: i for i, k in enumerate(df[\"category\"].unique())}\n",
    "    y = df[\"category\"].map(category_to_idx).astype(\"float32\")\n",
    "\n",
    "    # Get both representations\n",
    "    X_tfidf = get_embeddings(X, method=\"tfidf\")\n",
    "    X_seq = get_embeddings(X, method=\"sequence\")\n",
    "\n",
    "    # Split both\n",
    "    X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(\n",
    "        X_tfidf, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_seq_train, X_seq_test, _, _ = train_test_split(\n",
    "        X_seq, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Data split into training and testing sets successfully.\")\n",
    "    return X_tfidf_train, X_tfidf_test, X_seq_train, X_seq_test, y_train, y_test\n",
    "\n",
    "\n",
    "def clean_text(X):\n",
    "    X = X.str.lower()\n",
    "    X = X.str.replace(r\"http\\S+|www\\S+|https\\S+\", \"\", regex=True)\n",
    "    X = X.str.replace(r\"\\@\\w+|\\#\", \"\", regex=True)\n",
    "    # Keep numbers and some punctuation\n",
    "    X = X.str.replace(r\"[^a-zA-Z0-9\\s\\.\\?\\!]\", \"\", regex=True)\n",
    "    X = X.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    return X.str.strip()\n",
    "\n",
    "\n",
    "def get_embeddings(X, method=\"tfidf\", max_features=50000, max_len=500):\n",
    "    if method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "            stop_words=\"english\",\n",
    "            min_df=5,\n",
    "            max_df=0.7,\n",
    "        )\n",
    "        return vectorizer.fit_transform(X)\n",
    "\n",
    "    elif method == \"sequence\":\n",
    "        tokenizer = Tokenizer(\n",
    "            num_words=max_features,\n",
    "            oov_token=\"<OOV>\",\n",
    "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        )\n",
    "        tokenizer.fit_on_texts(X)\n",
    "        sequences = tokenizer.texts_to_sequences(X)\n",
    "        return pad_sequences(\n",
    "            sequences, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src.main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-15T20:15:22.582387Z",
     "iopub.status.busy": "2025-08-15T20:15:22.582106Z",
     "iopub.status.idle": "2025-08-15T22:50:23.512100Z",
     "shell.execute_reply": "2025-08-15T22:50:23.511310Z",
     "shell.execute_reply.started": "2025-08-15T20:15:22.582366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Main ********************\n",
      "\n",
      "Step 1: Loading dataset...\n",
      "Data loaded successfully.\n",
      "\n",
      "Step 2: Preprocessing data...\n",
      "Data split into training and testing sets successfully.\n",
      "Data preprocessed successfully.\n",
      "\n",
      "Step 3: Training and Evaluating models...\n",
      "\n",
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model training completed.\n",
      "Training Accuracy: 0.8272\n",
      "Logistic Regression Evaluation Accuracy: 0.7379\n",
      "\n",
      "Training Random Forest model...\n",
      "Random Forest model training completed.\n",
      "Training Accuracy: 1.0000\n",
      "Random Forest Evaluation Accuracy: 0.6759\n",
      "\n",
      "Training Naive Bayes model...\n",
      "Naive Bayes model training completed.\n",
      "Training Accuracy: 0.6296\n",
      "Naive Bayes Evaluation Accuracy: 0.5903\n",
      "\n",
      "Training XGBoost model...\n",
      "XGBoost model training completed.\n",
      "Training Accuracy: 0.8021\n",
      "XGBoost Evaluation Accuracy: 0.6960\n",
      "\n",
      "Training Simple Feedforward model...\n",
      "Epoch 1/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - accuracy: 0.1533 - loss: 159.1739 - val_accuracy: 0.1714 - val_loss: 3.2882\n",
      "Epoch 2/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1711 - loss: 3.3031 - val_accuracy: 0.1714 - val_loss: 3.2875\n",
      "Epoch 3/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1711 - loss: 3.2951 - val_accuracy: 0.1714 - val_loss: 3.2876\n",
      "Epoch 4/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1696 - loss: 3.2925 - val_accuracy: 0.1714 - val_loss: 3.2874\n",
      "Epoch 5/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1706 - loss: 3.2957 - val_accuracy: 0.1714 - val_loss: 3.2875\n",
      "Epoch 6/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1714 - loss: 3.2900 - val_accuracy: 0.1714 - val_loss: 3.2874\n",
      "Epoch 7/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1708 - loss: 3.2870 - val_accuracy: 0.1714 - val_loss: 3.2877\n",
      "Epoch 8/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1713 - loss: 3.2850 - val_accuracy: 0.1714 - val_loss: 3.2875\n",
      "Epoch 9/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1702 - loss: 3.2880 - val_accuracy: 0.1714 - val_loss: 3.2873\n",
      "Epoch 10/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.1702 - loss: 3.2879 - val_accuracy: 0.1714 - val_loss: 3.2876\n",
      "Simple Feedforward model training completed.\n",
      "Epoch 1: Training Accuracy = 0.1663\n",
      "Epoch 2: Training Accuracy = 0.1706\n",
      "Epoch 3: Training Accuracy = 0.1706\n",
      "Epoch 4: Training Accuracy = 0.1706\n",
      "Epoch 5: Training Accuracy = 0.1706\n",
      "Epoch 6: Training Accuracy = 0.1706\n",
      "Epoch 7: Training Accuracy = 0.1706\n",
      "Epoch 8: Training Accuracy = 0.1706\n",
      "Epoch 9: Training Accuracy = 0.1706\n",
      "Epoch 10: Training Accuracy = 0.1706\n",
      "\n",
      "Evaluating model: sequential_1\n",
      "\u001b[1m1310/1310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "sequential_1 Accuracy: 0.1669\n",
      "Simple Feedforward Evaluation Accuracy: 0.1669\n",
      "\n",
      "Training Stacked LSTM model...\n",
      "Epoch 1/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 52ms/step - accuracy: 0.1687 - loss: 3.3176 - val_accuracy: 0.1714 - val_loss: 3.2892\n",
      "Epoch 2/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 52ms/step - accuracy: 0.1699 - loss: 3.2948 - val_accuracy: 0.1714 - val_loss: 3.2891\n",
      "Epoch 3/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 52ms/step - accuracy: 0.1701 - loss: 3.2906 - val_accuracy: 0.1714 - val_loss: 3.2886\n",
      "Epoch 4/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 52ms/step - accuracy: 0.1706 - loss: 3.2924 - val_accuracy: 0.1714 - val_loss: 3.2888\n",
      "Epoch 5/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 51ms/step - accuracy: 0.1714 - loss: 3.2888 - val_accuracy: 0.1714 - val_loss: 3.2885\n",
      "Epoch 6/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 51ms/step - accuracy: 0.1691 - loss: 3.2941 - val_accuracy: 0.1714 - val_loss: 3.2883\n",
      "Epoch 7/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 51ms/step - accuracy: 0.1721 - loss: 3.2860 - val_accuracy: 0.1714 - val_loss: 3.2884\n",
      "Epoch 8/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 51ms/step - accuracy: 0.1704 - loss: 3.2921 - val_accuracy: 0.1714 - val_loss: 3.2888\n",
      "Epoch 9/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 51ms/step - accuracy: 0.1716 - loss: 3.2840 - val_accuracy: 0.1714 - val_loss: 3.2883\n",
      "Epoch 10/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 51ms/step - accuracy: 0.1699 - loss: 3.2892 - val_accuracy: 0.1714 - val_loss: 3.2879\n",
      "Stacked LSTM model training completed.\n",
      "Epoch 1: Training Accuracy = 0.1701\n",
      "Epoch 2: Training Accuracy = 0.1706\n",
      "Epoch 3: Training Accuracy = 0.1706\n",
      "Epoch 4: Training Accuracy = 0.1706\n",
      "Epoch 5: Training Accuracy = 0.1706\n",
      "Epoch 6: Training Accuracy = 0.1706\n",
      "Epoch 7: Training Accuracy = 0.1706\n",
      "Epoch 8: Training Accuracy = 0.1706\n",
      "Epoch 9: Training Accuracy = 0.1706\n",
      "Epoch 10: Training Accuracy = 0.1706\n",
      "\n",
      "Evaluating model: functional_5\n",
      "\u001b[1m1310/1310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 21ms/step\n",
      "functional_5 Accuracy: 0.1669\n",
      "Stacked LSTM Evaluation Accuracy: 0.1669\n",
      "\n",
      "Training GRU model...\n",
      "Epoch 1/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 25ms/step - accuracy: 0.1675 - loss: 3.3286 - val_accuracy: 0.1714 - val_loss: 3.2904\n",
      "Epoch 2/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1710 - loss: 3.2959 - val_accuracy: 0.1714 - val_loss: 3.2893\n",
      "Epoch 3/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1711 - loss: 3.2905 - val_accuracy: 0.1714 - val_loss: 3.2879\n",
      "Epoch 4/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1696 - loss: 3.2911 - val_accuracy: 0.1714 - val_loss: 3.2883\n",
      "Epoch 5/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1698 - loss: 3.2934 - val_accuracy: 0.1714 - val_loss: 3.2888\n",
      "Epoch 6/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1719 - loss: 3.2872 - val_accuracy: 0.1714 - val_loss: 3.2883\n",
      "Epoch 7/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1700 - loss: 3.2905 - val_accuracy: 0.1714 - val_loss: 3.2881\n",
      "Epoch 8/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1687 - loss: 3.2923 - val_accuracy: 0.1714 - val_loss: 3.2924\n",
      "Epoch 9/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1695 - loss: 3.2921 - val_accuracy: 0.1714 - val_loss: 3.2876\n",
      "Epoch 10/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 25ms/step - accuracy: 0.1704 - loss: 3.2881 - val_accuracy: 0.1714 - val_loss: 3.2889\n",
      "GRU model training completed.\n",
      "Epoch 1: Training Accuracy = 0.1700\n",
      "Epoch 2: Training Accuracy = 0.1706\n",
      "Epoch 3: Training Accuracy = 0.1706\n",
      "Epoch 4: Training Accuracy = 0.1706\n",
      "Epoch 5: Training Accuracy = 0.1706\n",
      "Epoch 6: Training Accuracy = 0.1706\n",
      "Epoch 7: Training Accuracy = 0.1706\n",
      "Epoch 8: Training Accuracy = 0.1706\n",
      "Epoch 9: Training Accuracy = 0.1706\n",
      "Epoch 10: Training Accuracy = 0.1706\n",
      "\n",
      "Evaluating model: functional_6\n",
      "\u001b[1m1310/1310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step\n",
      "functional_6 Accuracy: 0.1669\n",
      "GRU Evaluation Accuracy: 0.1669\n",
      "\n",
      "Training Transformer model...\n",
      "Epoch 1/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 20ms/step - accuracy: 0.3990 - loss: 2.4061 - val_accuracy: 0.6866 - val_loss: 1.1536\n",
      "Epoch 2/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.7048 - loss: 1.0767 - val_accuracy: 0.7232 - val_loss: 1.0122\n",
      "Epoch 3/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.7630 - loss: 0.8342 - val_accuracy: 0.7294 - val_loss: 1.0199\n",
      "Epoch 4/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.7953 - loss: 0.6973 - val_accuracy: 0.7296 - val_loss: 1.0205\n",
      "Epoch 5/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.8189 - loss: 0.6026 - val_accuracy: 0.7318 - val_loss: 1.1086\n",
      "Epoch 6/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.8387 - loss: 0.5232 - val_accuracy: 0.7234 - val_loss: 1.1528\n",
      "Epoch 7/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.8540 - loss: 0.4706 - val_accuracy: 0.7220 - val_loss: 1.2354\n",
      "Epoch 8/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.8666 - loss: 0.4206 - val_accuracy: 0.7209 - val_loss: 1.2746\n",
      "Epoch 9/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.8738 - loss: 0.3936 - val_accuracy: 0.7217 - val_loss: 1.3540\n",
      "Epoch 10/10\n",
      "\u001b[1m4715/4715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 20ms/step - accuracy: 0.8823 - loss: 0.3610 - val_accuracy: 0.7159 - val_loss: 1.4300\n",
      "Transformer model training completed.\n",
      "Epoch 1: Training Accuracy = 0.5361\n",
      "Epoch 2: Training Accuracy = 0.7094\n",
      "Epoch 3: Training Accuracy = 0.7562\n",
      "Epoch 4: Training Accuracy = 0.7875\n",
      "Epoch 5: Training Accuracy = 0.8096\n",
      "Epoch 6: Training Accuracy = 0.8280\n",
      "Epoch 7: Training Accuracy = 0.8422\n",
      "Epoch 8: Training Accuracy = 0.8535\n",
      "Epoch 9: Training Accuracy = 0.8638\n",
      "Epoch 10: Training Accuracy = 0.8722\n",
      "\n",
      "Evaluating model: functional_7\n",
      "\u001b[1m1310/1310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step\n",
      "functional_7 Accuracy: 0.7207\n",
      "Transformer Evaluation Accuracy: 0.7207\n",
      "Evaluation Accuracy Results: {'Logistic Regression': 0.7379423907596114, 'Random Forest': 0.6758943273751282, 'Naive Bayes': 0.5902918645443047, 'XGBoost': 0.6959644894160323, 'Simple Feedforward': 0.1668615612247333, 'Stacked LSTM': 0.1668615612247333, 'GRU': 0.1668615612247333, 'Transformer': 0.7206643915710093}\n",
      "Models Trained and Evaluated successfully.\n",
      "******************** Return ********************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"*\" * 20, \"Main\", \"*\" * 20)\n",
    "    print(\"\\nStep 1: Loading dataset...\")\n",
    "    with open(\"/kaggle/input/news-category-dataset/News_Category_Dataset_v3.json\", \"r\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    # convert to pandas DataFrame\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    # data = data.head(100) # For testing\n",
    "\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Step 2: Preprocess text\n",
    "    print(\"\\nStep 2: Preprocessing data...\")\n",
    "    X_tfidf_train, X_tfidf_test, X_seq_train, X_seq_test, y_train, y_test = (\n",
    "        preprocess_data(data)\n",
    "    )\n",
    "    print(\"Data preprocessed successfully.\")\n",
    "\n",
    "    # Step 3: Train models\n",
    "    print(\"\\nStep 3: Training and Evaluating models...\")\n",
    "    models = train_and_evaluate_models(\n",
    "        X_tfidf_train, X_tfidf_test, X_seq_train, X_seq_test, y_train, y_test, num_classes=data[\"category\"].nunique()\n",
    "    )\n",
    "    print(\"Models Trained and Evaluated successfully.\")\n",
    "    print(\"*\" * 20, \"Return\", \"*\" * 20)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 32526,
     "sourceId": 4243451,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Task-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
