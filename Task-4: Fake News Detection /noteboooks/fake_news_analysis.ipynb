{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake News Detection Analysis\n",
        "\n",
        "This notebook provides a comprehensive analysis of fake news detection using various machine learning models.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading and Exploration](#data-loading)\n",
        "2. [Data Preprocessing](#data-preprocessing)\n",
        "3. [Model Training and Comparison](#model-training)\n",
        "4. [Results Analysis](#results-analysis)\n",
        "5. [Feature Analysis](#feature-analysis)\n",
        "6. [Model Deployment](#model-deployment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration {#data-loading}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Import our custom modules\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "sys.path.append('../model')\n",
        "\n",
        "from preprocess_data import FakeNewsPreprocessor\n",
        "from get_model import get_model, get_model_info, compare_models\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "fake_path = \"../data/fake.csv\"\n",
        "true_path = \"../data/true.csv\"\n",
        "\n",
        "preprocessor = FakeNewsPreprocessor()\n",
        "df = preprocessor.load_data(fake_path, true_path)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(\"=\" * 50)\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(\"=\" * 50)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(\"=\" * 30)\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check label distribution\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(\"=\" * 30)\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nFake news: {df['label'].value_counts()[0]} ({df['label'].value_counts()[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"True news: {df['label'].value_counts()[1]} ({df['label'].value_counts()[1]/len(df)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing and Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the data and train models\n",
        "print(\"Starting data preprocessing...\")\n",
        "df_processed = preprocessor.preprocess_data(df)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = preprocessor.split_data(df_processed)\n",
        "\n",
        "# Vectorize text for traditional ML models\n",
        "X_train_tfidf, X_test_tfidf = preprocessor.vectorize_text(X_train, X_test)\n",
        "\n",
        "print(f\"Training set: {X_train_tfidf.shape}\")\n",
        "print(f\"Test set: {X_test_tfidf.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and compare multiple models\n",
        "models_to_test = ['logistic', 'svm', 'random_forest', 'naive_bayes']\n",
        "results = {}\n",
        "\n",
        "for model_type in models_to_test:\n",
        "    print(f\"\\nTraining {model_type} model...\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Create and train model\n",
        "    model = get_model(model_type)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    from sklearn.metrics import accuracy_score, classification_report\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    results[model_type] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Fake', 'True']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model accuracies\n",
        "accuracies = [results[model]['accuracy'] for model in models_to_test]\n",
        "model_names = [get_model_info(model)['name'] for model in models_to_test]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0.8, 1.0)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
        "             f'{acc:.4f}', ha='center', va='bottom')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(model_names, accuracies, marker='o', linewidth=2, markersize=8)\n",
        "plt.title('Model Accuracy Trend')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0.8, 1.0)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best model\n",
        "best_model_type = models_to_test[np.argmax(accuracies)]\n",
        "best_accuracy = max(accuracies)\n",
        "print(f\"\\nBest Model: {get_model_info(best_model_type)['name']} with accuracy: {best_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a prediction function\n",
        "def predict_news(text, model, preprocessor):\n",
        "    \"\"\"\n",
        "    Predict if a news article is fake or true\n",
        "    \n",
        "    Args:\n",
        "        text (str): News article text\n",
        "        model: Trained model\n",
        "        preprocessor: Trained preprocessor\n",
        "    \n",
        "    Returns:\n",
        "        dict: Prediction results\n",
        "    \"\"\"\n",
        "    # Clean the text\n",
        "    cleaned_text = preprocessor.clean_text(text)\n",
        "    \n",
        "    # Vectorize\n",
        "    text_vectorized = preprocessor.tfidf_vectorizer.transform([cleaned_text])\n",
        "    \n",
        "    # Predict\n",
        "    prediction = model.predict(text_vectorized)[0]\n",
        "    \n",
        "    # Get probability if available\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        probability = model.predict_proba(text_vectorized)[0]\n",
        "        confidence = max(probability)\n",
        "    else:\n",
        "        confidence = None\n",
        "    \n",
        "    return {\n",
        "        'prediction': 'Fake' if prediction == 0 else 'True',\n",
        "        'confidence': confidence,\n",
        "        'fake_probability': probability[0] if confidence else None,\n",
        "        'true_probability': probability[1] if confidence else None\n",
        "    }\n",
        "\n",
        "# Test the prediction function\n",
        "sample_text = \"This is a test news article to see how our model performs.\"\n",
        "best_model = results[best_model_type]['model']\n",
        "result = predict_news(sample_text, best_model, preprocessor)\n",
        "\n",
        "print(\"Sample Prediction:\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Text: {sample_text}\")\n",
        "print(f\"Prediction: {result['prediction']}\")\n",
        "if result['confidence']:\n",
        "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "    print(f\"Fake Probability: {result['fake_probability']:.4f}\")\n",
        "    print(f\"True Probability: {result['true_probability']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
