{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-06T14:26:46.955182Z",
     "iopub.status.busy": "2025-08-06T14:26:46.954466Z",
     "iopub.status.idle": "2025-08-06T14:26:47.098450Z",
     "shell.execute_reply": "2025-08-06T14:26:47.097661Z",
     "shell.execute_reply.started": "2025-08-06T14:26:46.955148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models/get_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T14:26:47.100168Z",
     "iopub.status.busy": "2025-08-06T14:26:47.099810Z",
     "iopub.status.idle": "2025-08-06T14:26:47.109187Z",
     "shell.execute_reply": "2025-08-06T14:26:47.108383Z",
     "shell.execute_reply.started": "2025-08-06T14:26:47.100143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Input,\n",
    "    GlobalAveragePooling1D,\n",
    "    MultiHeadAttention,\n",
    "    LayerNormalization,\n",
    "    Add,\n",
    ")\n",
    "\n",
    "\n",
    "def get_machine_learning_models():\n",
    "    \"\"\"\n",
    "    Returns a dictionary of machine learning models with their names as keys.\n",
    "    \"\"\"\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"Naive Bayes\": MultinomialNB(),\n",
    "    }\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def get_deep_learning_models(vocab_size=20000, max_len=500, embed_dim=128, num_heads=4):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of deep learning models with their names as keys.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # 1. Simple Feedforward\n",
    "    models[\"Simple Feedforward\"] = Sequential(\n",
    "        [\n",
    "            Dense(128, activation=\"relu\", input_shape=(500,)),\n",
    "            Dropout(0.5),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 2. LSTM Model\n",
    "    lstm_input = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(lstm_input)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "    models[\"LSTM\"] = Model(inputs=lstm_input, outputs=output)\n",
    "\n",
    "    # 3. GRU Model\n",
    "    gru_input = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(gru_input)\n",
    "    x = GRU(64)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "    models[\"GRU\"] = Model(inputs=gru_input, outputs=output)\n",
    "\n",
    "    # 4. Transformer-like Model (simple attention block)\n",
    "    trans_input = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(trans_input)\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "    models[\"Transformer-Attention\"] = Model(inputs=trans_input, outputs=output)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/preprocess_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T14:26:47.110409Z",
     "iopub.status.busy": "2025-08-06T14:26:47.110158Z",
     "iopub.status.idle": "2025-08-06T14:26:47.130466Z",
     "shell.execute_reply": "2025-08-06T14:26:47.129760Z",
     "shell.execute_reply.started": "2025-08-06T14:26:47.110387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.dropna().drop_duplicates()\n",
    "    X = clean_text(df[\"review\"])\n",
    "    y = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0}).astype(\"float32\")\n",
    "\n",
    "    # Get both representations\n",
    "    X_tfidf = get_embeddings(X, method=\"tfidf\")\n",
    "    X_seq = get_embeddings(X, method=\"sequence\")\n",
    "\n",
    "    # Split both\n",
    "    X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(\n",
    "        X_tfidf, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_seq_train, X_seq_test, _, _ = train_test_split(\n",
    "        X_seq, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"Data split into training and testing sets successfully.\")\n",
    "    return X_tfidf_train, X_tfidf_test, X_seq_train, X_seq_test, y_train, y_test\n",
    "\n",
    "\n",
    "def clean_text(X):\n",
    "    # Example cleaning function, modify as needed\n",
    "    X = X.str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
    "    print(\"Text data cleaned successfully.\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_embeddings(X, method=\"tfidf\", max_features=20000, max_len=500):\n",
    "    if method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        X_vec = vectorizer.fit_transform(X)\n",
    "        print(\"TF-IDF embeddings generated successfully.\")\n",
    "        return X_vec\n",
    "\n",
    "    elif method == \"sequence\":\n",
    "        tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "        tokenizer.fit_on_texts(X)\n",
    "        sequences = tokenizer.texts_to_sequences(X)\n",
    "        padded = pad_sequences(\n",
    "            sequences, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",
    "        )\n",
    "        print(\"Sequence embeddings generated successfully.\")\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/train_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T14:26:47.131971Z",
     "iopub.status.busy": "2025-08-06T14:26:47.131758Z",
     "iopub.status.idle": "2025-08-06T14:26:47.149220Z",
     "shell.execute_reply": "2025-08-06T14:26:47.148587Z",
     "shell.execute_reply.started": "2025-08-06T14:26:47.131955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_models(X_tfidf_train, y_train, X_seq_train):\n",
    "    \"\"\"\n",
    "    Trains and returns a dictionary of machine learning and deep learning models.\n",
    "    \"\"\"\n",
    "    ml_models = get_machine_learning_models()\n",
    "    dl_models = get_deep_learning_models()\n",
    "\n",
    "    # Train machine learning models on TF-IDF features\n",
    "    for name, model in ml_models.items():\n",
    "        print(f\"Training {name} model...\")\n",
    "        model.fit(X_tfidf_train, y_train)\n",
    "        print(f\"{name} model trained successfully.\")\n",
    "\n",
    "    # Train deep learning models on sequence data\n",
    "    for name, model in dl_models.items():\n",
    "        print(f\"Training {name} model...\")\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "        model.fit(X_seq_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "        print(f\"{name} model trained successfully.\")\n",
    "\n",
    "    print(\"All models trained successfully.\")\n",
    "    return {**ml_models, **dl_models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T14:26:47.150240Z",
     "iopub.status.busy": "2025-08-06T14:26:47.150030Z",
     "iopub.status.idle": "2025-08-06T14:26:47.166635Z",
     "shell.execute_reply": "2025-08-06T14:26:47.166104Z",
     "shell.execute_reply.started": "2025-08-06T14:26:47.150215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Evaluates the given model on the test data and returns the accuracy.\n",
    "\n",
    "    Args:\n",
    "        model: A trained scikit-learn or Keras model.\n",
    "        test_data: The input data for testing.\n",
    "        test_labels: The true labels for testing.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the test data.\n",
    "    \"\"\"\n",
    "    model_name = model.name if isinstance(model, Model) else model.__class__.__name__\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    if hasattr(model, \"predict\"):\n",
    "        if isinstance(model, Model):  # Keras model\n",
    "            predictions = model.predict(test_data, verbose=0)\n",
    "\n",
    "            # Convert probabilities to binary class labels (sigmoid output)\n",
    "            if predictions.shape[-1] == 1:\n",
    "                predictions = (predictions > 0.5).astype(\"int32\").flatten()\n",
    "            else:\n",
    "                predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "        else:  # Scikit-learn model\n",
    "            predictions = model.predict(test_data)\n",
    "\n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        print(f\"{model_name} Accuracy: {accuracy:.2f}\")\n",
    "        return accuracy\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/visualize_most_freq_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_most_frequent_words(df):\n",
    "    positive_reviews = df[df[\"sentiment\"] == \"positive\"][\"review\"].str.cat(sep=\" \")\n",
    "    negative_reviews = df[df[\"sentiment\"] == \"negative\"][\"review\"].str.cat(sep=\" \")\n",
    "\n",
    "    wordcloud_pos = WordCloud(width=800, height=400, background_color=\"white\").generate(\n",
    "        positive_reviews\n",
    "    )\n",
    "    wordcloud_neg = WordCloud(\n",
    "        width=800, height=400, background_color=\"black\", colormap=\"Reds\"\n",
    "    ).generate(negative_reviews)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Most Frequent Positive Words\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Most Frequent Negative Words\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T14:26:47.167779Z",
     "iopub.status.busy": "2025-08-06T14:26:47.167541Z",
     "iopub.status.idle": "2025-08-06T14:42:52.348571Z",
     "shell.execute_reply": "2025-08-06T14:42:52.347898Z",
     "shell.execute_reply.started": "2025-08-06T14:26:47.167757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"*\" * 20, \"Main\", \"*\" * 20)\n",
    "\n",
    "    # Step 1: Download data\n",
    "    # data_downloader()\n",
    "    # print(\"Data downloaded successfully.\")\n",
    "\n",
    "    # Step 2: Load and limit dataset (for debugging or testing)\n",
    "    df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "    # df = df.head(10)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Step 3: Preprocess text → returns TF-IDF and Sequence embeddings\n",
    "    X_tfidf_train, X_tfidf_test, X_seq_train, X_seq_test, y_train, y_test = (\n",
    "        preprocess_data(df)\n",
    "    )\n",
    "    print(\"Data preprocessed successfully.\")\n",
    "\n",
    "    # Step 4: Train all models\n",
    "    models = train_models(X_tfidf_train, y_train, X_seq_train)\n",
    "    print(\"Models trained successfully.\")\n",
    "\n",
    "    # Step 5: Evaluate each model using appropriate test input\n",
    "    accuracy_results = {}\n",
    "    for name, model in models.items():\n",
    "        if name in [\"Logistic Regression\", \"Random Forest\", \"Naive Bayes\"]:\n",
    "            X_test = X_tfidf_test\n",
    "        else:\n",
    "            X_test = X_seq_test\n",
    "\n",
    "        accuracy = evaluate_model(model, X_test, y_test)\n",
    "        accuracy_results[name] = accuracy\n",
    "        print(f\"{name} Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    print(\"Model evaluation completed.\")\n",
    "    print(\"Accuracy Results:\", accuracy_results)\n",
    "\n",
    "    # Visualize the most frequent positive and negative words\n",
    "    visualize_most_frequent_words(df)\n",
    "    print(\"*\" * 20, \"Return\", \"*\" * 20)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 333307,
     "datasetId": 134715,
     "isSourceIdPinned": false,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
